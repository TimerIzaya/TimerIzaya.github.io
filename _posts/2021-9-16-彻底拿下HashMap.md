---
layout:     post
title:      彻底拿下HashMap和ConcurrentHashMap（待续）
subtitle:   复盘学习顶级JAVA大佬的思维
date:       2021-9-16
author:     Timer
header-img: img/the-first.png
catalog: false
tags:
    - JDK


---

## 前提： HashMap在JDK中的结构层次

先来一张Java.Util包的类结构（可打开大图）

![](https://gitee.com/timerizaya/timer-pic/raw/master/img/Package%20util.png)

Util里主要容器类就是Collection和Map，先研究Map这一块，Map容器系列设计结构如下。

![](https://gitee.com/timerizaya/timer-pic/raw/master/img/20210916153537.png)

#### Map：Map接口设计好了所有Map基础操作的方法。

#### AbstractMap：

​    **关键注释：This class provides a skeletal implementation of the Map interface, to minimize the effort required to implement this interface.To implement an unmodifiable map, the programmer needs only to extend this class and provide an implementation for the entrySet method, which returns a set-view of the map's mappings.  To implement a modifiable map, the programmer must additionally override this class's put method (which otherwise throws an UnsupportedOperationException)。**

**简要概括：这个类提供了Map接口的简单实现，目的是让后续实现这个类的时候少费点功夫。**

AbstractMap仅有一个抽象方法：

![](https://gitee.com/timerizaya/timer-pic/raw/master/img/20210916162353.png)

要想实现一个不可修改的Map（视图Map），只需要继承这个类，实现这个方法，这样就可以返回这个map的视图了。

要想实现一个可修改的Map（正常使用的Map），就必须要实现这个类的put方法。因为AbstractMap默认自己是个视图Map。



**看源码看到这里，想吐槽一点，JDK里有的不可变Map，也就是Collections.UnmodifiableMap。只实现了Map接口，并没有继承这个类，两个类的作者是一个人，自己都不用自己写的类，离谱。**

```java
 private static class UnmodifiableMap<K,V> implements Map<K,V>, Serializable {
     //实现
 }
```

**所以到头来，Abstract的作用就只在于：实现了很多Map的基础方法，子类的其他Map，比如Hashmap、TreeMap等，可以选择性的覆盖。** 

但是实际上呢？HashMap、TreeMap，真的用到了AbstractMap里写的方法了吗？

Map里最关键的get()和put()方法，put()没写，get()也仅仅是循环Entry找到目标Key，和没写一样。

去StackOverFlow翻了一圈，有个类似的问题：

**Why does LinkedHashSet<E> extend HashSet<e> and implement Set<E>**

下面有个JDK作者的朋友回复了：

**I've asked Josh Bloch, and he informs me that it was a mistake. He used to think, long ago, that there was some value in it, but he since "saw the light". Clearly JDK maintainers haven't considered this to be worth backing out later.**

"Mistake"解决了所有疑问。所以，HashMap其实只实现Map接口就行了，不用考虑AbstractMap。   



# 正篇：

## 1.HashMap总体结构

从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的。

<img src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2016/e4a19398.png" alt="img" style="zoom: 33%;" /> 

1. 其中黑色的节点就是源码中的Node，整个数组就是Node[] table。

   ```java
       static class Node<K,V> implements Map.Entry<K,V> {
           final int hash; //用来确定其在table中的索引
           final K key;
           V value;
           Node<K,V> next; //链表的下一个node
       }
   ```

2. 对于整体table，有几个关键的变量需要知道。

   ```
        int length;			   // table总长度
        int threshold;             // 所能容纳的key-value对极限 == length * loadFactor
        final float loadFactor;    // 负载因子
        int size;  			   // 实际node数量
   ```

   这里的length是2^n，而常规来说桶的大小应该是素数，相对来说**素数冲突的概率要小于合数**（流传结论，有待研究）。HashMap这样做的理由是方便**扩容**和**取模**，为了减少冲突，hash算法也加入了**高位参与运算**。具体见下文。

   这里还涉及到两个细节，为什么负载因子默认是**0.75**，为什么链表超过**8**才会升级成红黑树。

   - #### 负载因子

     搜遍StackOverFlow、Quora、Google各大能搜索的地方，看了无数争论和乱科普的帖子。

     我的结论是，没有任何特殊意义，纯属是一种简单的折中方案，0.5的话就要浪费一半空间，1的话不可行，所以取折中。

     贴一下维基百科HashMap词条的负载因子内容作为实锤：

     > The performance of the hash table worsens in relation to the load factor as **α** approaches 1. Hence, it's essential to resize—or "rehash"—the hash table when the load factor exceeds an ideal value. It's also efficient to resize the hash table if the size is smaller—which is usually done when load factor drops below **αmax / 4** .
     >
     > **Generally, a load factor of 0.6 and 0.75 is an acceptable figure.**

     **总的来说，要是负载因子够小，内存足够，那么哈希算法哪怕很垃圾，也能减少冲突。要是负载因子太大，那么就必须要求哈希算法足够完美和均匀，否则容易冲突。0.75是一种妥协。**    

     

   - #### 升级红黑树

     首先说一下泊松分布的定义：**泊松分布是单位时间内独立事件发生次数的概率分布**。公式这里不贴了，泊松分布一共两个重要的变量：λ和k。λ是单位时间（单位面积）内，随机事件的平均发生率。k是发生的次数。

     这里带入table，λ的定义是，用户put进某个bin的概率，k是用户put进同个桶k次的概率。

     这里其实不管λ是多少，比如0.6、0.7，泊松分布所计算得进入同个桶8次的概率都低的离谱，为千万分之一。这里贴一下λ取几个常用值的表。源码注释中默认λ是0.5。

     | λ     | k = 0      | k = 1      | k = 2      | k = 3      | k = 4      | k = 5      | k = 6      | k = 7      | k = 8      |
     | ----- | :--------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |
     | 0.5   | 0.60653066 | 0.30326533 | 0.07581633 | 0.01263606 | 0.00157951 | 0.00015795 | 0.00001316 | 0.00000094 | 0.00000006 |
     | 0.56  | 0.57120906 | 0.31987708 | 0.08956558 | 0.01671891 | 0.00234065 | 0.00026215 | 0.00002447 | 0.00000196 | 0.00000014 |
     | 0.375 | 0.68728928 | 0.25773348 | 0.04832503 | 0.00604063 | 0.00056631 | 0.00004247 | 0.00000266 | 0.00000014 | 0.00000001 |
     | 0.6   | 0.54881164 | 0.32928698 | 0.09878609 | 0.01975722 | 0.00296358 | 0.00035563 | 0.00003556 | 0.00000305 | 0.00000023 |
     | 0.7   | 0.49658530 | 0.34760971 | 0.12166340 | 0.02838813 | 0.00496792 | 0.00069551 | 0.00008114 | 0.00000811 | 0.00000071 |

     但是源码中的解释很值得玩味：

     > Ideally, under random hashCodes, the frequency of nodes in bins follows a Poisson distribution with a parameter of about 0.5 on average for the default resizing threshold of 0.75, although with a large variance because of resizing granularity.

     他说λ为0.5是因为threashold是0.75，取一个大概的值，具体0.5怎么来的没说明白，网上众说纷纭，下面我来说一下我的理解和证明。

     ​                                          <img src="https://gitee.com/timerizaya/timer-pic/raw/master/img/image-20211115200220059.png" alt="image-20211115200220059" style="zoom:50%;" /> 

     

     作者只带入了扩容一次的λ，也就是0.5，实际项目中，基本都是预估好map的capacity，不会扩容太多次，毕竟创建大数组消耗的时间是相当庞大的。

     事实上，以f为0.75举例，当λ接近0.375的时候，bin里链表的长度为8的概率比0.5还要小六倍，数据可见上表格。这也解释了f为什么越小，越不容易冲突。

     **最后，由分析可得，链表为8的概率仅仅为千万分之一，红黑树并不是一种性能优化，而是一种保底手段。**



#### 

































